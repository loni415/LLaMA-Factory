### model
model_name_or_path: mistralai/Mistral-7B-v0.3
trust_remote_code: false
use_fast_tokenizer: true

### method
stage: sft
finetuning_type: lora
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target: all

### data
dataset: my_sft_aug            # or: my_sft_sharegpt
dataset_dir: ./data
template: mistral              # match the model family; keep same at inference
cutoff_len: 4096
packing: true
neat_packing: true             # SFT-only “contamination-free” packing
preprocessing_num_workers: 1
preprocessing_batch_size: 500
overwrite_cache: true
tokenized_path: /Users/lukasfiller/dev/LLaMA-Factory/.cache/mistral_v03_sft_tokens

### output
output_dir: ./saves/mistral7b_v03/sft_lora
logging_steps: 10
save_steps: 1000
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 1.0e-4
lr_scheduler_type: cosine
warmup_ratio: 0.03
num_train_epochs: 2.0
max_grad_norm: 1.0

# Apple M-series safe precision
fp16: false
bf16: false
disable_gradient_checkpointing: false

### eval (optional)
val_size: 0.05
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 1000

# optional: enable CPU fallback & lift MPS soft cap
!export PYTORCH_ENABLE_MPS_FALLBACK=1
!export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0

enter in cli to run
#llamafactory-cli train /Users/lukasfiller/dev/LLaMA-Factory/sft_mistral_lora.yaml