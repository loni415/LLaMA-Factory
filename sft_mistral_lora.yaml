### model

model_name_or_path: /Users/lukasfiller/dev/LLaMA-Factory/saves/mistral7b_v03/base
adapter_name_or_path: /Users/lukasfiller/dev/LLaMA-Factory/saves/mistral7b_v03/pt_lora/checkpoint-2000

trust_remote_code: true
use_fast_tokenizer: true

### method
stage: sft
finetuning_type: lora
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target: [q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj]

### data
dataset: sc_1_qa_merged        # Change from alpaca_en to your dataset
dataset_dir: ./data            # Keep this the same
template: mistral              # Keep this the same
#formatting: sharegpt           # Add this line to specify sharegpt format
cutoff_len: 4096
packing: true
neat_packing: true             # SFT-only “contamination-free” packing
preprocessing_num_workers: 1
preprocessing_batch_size: 500
overwrite_cache: true
tokenized_path: /Users/lukasfiller/dev/LLaMA-Factory/.cache/mistral_v03_sft_tokens

### output
output_dir: ./saves/mistral7b_v03/sft_lora
logging_steps: 10
save_steps: 1000
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 5.0e-5
lr_scheduler_type: cosine
warmup_ratio: 0.03
num_train_epochs: 2.0
max_grad_norm: 1.0
do_train: true

fp16: false
bf16: true
pure_bf16: false
infer_dtype: float16
gradient_checkpointing: true

### eval (optional)
val_size: 0.05
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 1000

optim: adamw_torch