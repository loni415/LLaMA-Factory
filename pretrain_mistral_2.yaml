### model
model_name_or_path: mistralai/Mistral-7B-v0.3
use_fast_tokenizer: true
low_cpu_mem_usage: true
flash_attn: disabled           # MPS doesn't support FA kernels
trust_remote_code: true

### method
stage: pt                      # pretraining / continued pretraining
do_train: true
finetuning_type: lora
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.05
#lora_target: all               # simple + effective on Mistral blocks

### data
dataset: augmentoolkit_pretrain
dataset_dir: ./data            # default; can be omitted, but explicit is fine
packing: true                  # PT packs sequences by default; keep it on
cutoff_len: 2048  #4096
val_size: 0.0
preprocessing_num_workers: 1
preprocessing_batch_size: 500
tokenized_path: /Users/lukasfiller/dev/LLaMA-Factory/.cache/mistral_v03_pt_tokens
#template:
overwrite_cache: true # set true once then back to false

#### --- saving / logging --- output
output_dir: ./saves/mistral7b_v03/pt_lora
plot_loss: true
overwrite_output_dir: true
save_strategy: steps         # save by steps (not epochs)
save_steps: 100              # pick a cadence youâ€™re happy to lose at most
save_total_limit: 3          # keep only the last 3 checkpoints
logging_steps: 10            # optional: frequent logs
load_best_model_at_end: false

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
learning_rate: 1.0e-4
weight_decay: 0.1
lr_scheduler_type: cosine
warmup_ratio: 0.03
max_grad_norm: 1.0
max_steps: 2000               # or set by your token budget
seed: 42

fp16: false                    # prefer bf16 on MPS?
bf16: true
pure_bf16: false
infer_dtype: float16
gradient_checkpointing: false
#disable_gradient_checkpointing: false
use_reentrant_gc: false
lora_target: [q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj]  # instead of "all"

optim: adamw_torch

resume_from_checkpoint: /Users/lukasfiller/dev/LLaMA-Factory/saves/mistral7b_v03/pt_lora/checkpoint-800

